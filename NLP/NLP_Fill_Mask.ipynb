{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fill-Mask\n",
        "คือการเติมคำในช่องว่าง\n",
        "ไม่ได้ถูกใช้เพื่อถามความรู้เพราะ model ไม่ได้ถูก train มาให้เข้าใจบริบทของคำถาม"
      ],
      "metadata": {
        "id": "ABEgs7rhNnhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "-1h4OIZKNpUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask = pipeline('fill-mask')\n",
        "print(fill_mask.model.name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck30O5CmOKM2",
        "outputId": "6441224b-f20e-4817-be19-ed10065a8f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distilbert/distilroberta-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a message with <mask>\n",
        "masked_message = \"I love you more than <mask>\""
      ],
      "metadata": {
        "id": "okuV6dJjN6fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the task\n",
        "output = fill_mask(masked_message)"
      ],
      "metadata": {
        "id": "mRLhHJn9OEl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the Output\n",
        "print(output)\n",
        "print(\"----------------\")\n",
        "for item in output:\n",
        "  print(f'{item['sequence']} , with score: {round(item['score'],4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A2iWGNSOHzh",
        "outputId": "14f8a582-1e0f-447a-e648-84454f85677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.2537836730480194, 'token': 655, 'token_str': ' ever', 'sequence': 'I love you more than ever'}, {'score': 0.14254993200302124, 'token': 47, 'token_str': ' you', 'sequence': 'I love you more than you'}, {'score': 0.04182794690132141, 'token': 932, 'token_str': ' anything', 'sequence': 'I love you more than anything'}, {'score': 0.021855279803276062, 'token': 4318, 'token_str': ' mine', 'sequence': 'I love you more than mine'}, {'score': 0.018760910257697105, 'token': 734, 'token_str': '...', 'sequence': 'I love you more than...'}]\n",
            "----------------\n",
            "I love you more than ever , with score: 0.2538\n",
            "I love you more than you , with score: 0.1425\n",
            "I love you more than anything , with score: 0.0418\n",
            "I love you more than mine , with score: 0.0219\n",
            "I love you more than... , with score: 0.0188\n"
          ]
        }
      ]
    }
  ]
}