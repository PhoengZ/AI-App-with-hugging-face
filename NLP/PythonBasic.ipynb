{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YHo0dE9sT5rq"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "223315d4d2d24ac5847a3930ef6b5add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e92a125b2a874377827a8bd4538362d5",
              "IPY_MODEL_f19045a25ea7484daf47fd4185a3afef",
              "IPY_MODEL_0e063c9c4d8d4952bfc8edd856dcca40"
            ],
            "layout": "IPY_MODEL_994f701ac2c44caf9a068f7335a7a968"
          }
        },
        "e92a125b2a874377827a8bd4538362d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1182cc09589496aaf3c7d6bd30189bd",
            "placeholder": "​",
            "style": "IPY_MODEL_7ee3b6221b944534a422b0f2e03c210d",
            "value": "model.safetensors: 100%"
          }
        },
        "f19045a25ea7484daf47fd4185a3afef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0efd3067e4de46059987d114fd187566",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_461bd200b82843d3b396cd3629101ea9",
            "value": 440449768
          }
        },
        "0e063c9c4d8d4952bfc8edd856dcca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99cb66f7cdd4344b377db00dcfdf047",
            "placeholder": "​",
            "style": "IPY_MODEL_5a3e00d720b6415aa3903e8a15e37a5d",
            "value": " 440M/440M [00:05&lt;00:00, 56.3MB/s]"
          }
        },
        "994f701ac2c44caf9a068f7335a7a968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1182cc09589496aaf3c7d6bd30189bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee3b6221b944534a422b0f2e03c210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0efd3067e4de46059987d114fd187566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461bd200b82843d3b396cd3629101ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d99cb66f7cdd4344b377db00dcfdf047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3e00d720b6415aa3903e8a15e37a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python เบื้องต้น"
      ],
      "metadata": {
        "id": "r-nqIR8LTB8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlDEHWK3S8QF",
        "outputId": "d2303fe8-07f8-4fa6-f10e-f6949ded0c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install transformer by specific version\n",
        "# !pip install transformers==4.41 (==version)"
      ],
      "metadata": {
        "id": "yV5avPBDTGit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU\n",
        "โดยเลือก Change Runtime แล้วเปลี่ยนประเภทรันไทม์"
      ],
      "metadata": {
        "id": "YHo0dE9sT5rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available()) # ใช้ตรวจสอบว่า GPU ถูกเปลี่ยนแล้วจริง"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7rV5jmZTtlz",
        "outputId": "db66abf2-f8c8-4566-937e-7267d4578cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "mnPiNvqWU55k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # ใช้ import ในการ import lib"
      ],
      "metadata": {
        "id": "tTbRZvw6U9OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline # ใช้ pipeline ในการเรียกใช้ model\n",
        "classfier = pipeline(\"sentiment-analysis\") # ชื่อ task จากนั้น pipelinme จะไปเลือก model ให้เหมาะกับงานเอง\n",
        "print(classfier('you fucking big!')[0]['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erqJeIJ3VFGS",
        "outputId": "77017e90-4c74-4ebe-b51d-361184b72046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การใช้งาน enumerate คู่กับ dict"
      ],
      "metadata": {
        "id": "JWqVOSLBpL-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fruits = ['apple', 'orange', 'banana']\n",
        "for i,fruit in enumerate(fruits):\n",
        "  print(i+1, \": \", fruit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Dd8cPiVWa6",
        "outputId": "027b356b-1e3d-4675-ce7c-9d43a715fac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 :  apple\n",
            "2 :  orange\n",
            "3 :  banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ใช้ dict.items() เพื่อทำให้ dict กลายเป็น list ที่มีคู่ของ key และ value ในแต่ละ index เป็นคู่ tuple\n",
        "vehicles = {'car': 100, 'motorcycle': 200, 'bycicle':50}\n",
        "for index,(key, value) in enumerate(vehicles.items()):\n",
        "  print(index+1,key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEPtfXwephM3",
        "outputId": "5a6f76b3-ced5-4931-f2e4-52ef3521bd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 car 100\n",
            "2 motorcycle 200\n",
            "3 bycicle 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ตัวอย่างการใช้งาน PIPELINE"
      ],
      "metadata": {
        "id": "11eyaQzprqxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('text-classification')\n",
        "result = classfier(\"I Love this course\")\n",
        "print(result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8ck9ajqruYV",
        "outputId": "26dd83a6-0896-4054-fb8b-670e6c31e512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.999883770942688}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ตัวอย่างการเรียกใช้งานคำสั่งในการดู model ที่ใช้\n",
        "print(classifier.model.name_or_path)\n",
        "# การดูว่า model นี้มีการจำแนกข้อความเป็นแบบใดบ้าง\n",
        "print(classifier.model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Od_ZoemsL00",
        "outputId": "cb913038-4e76-41b1-db83-651e0f53ddfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ตัวอย่างการกำหนด model\n",
        "classifier = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n",
        "result = classifier('I love this course!')\n",
        "print(\"label:\",result[0]['label'],\" score:\", round(result[0]['score'],4))\n",
        "print(classifier.model.name_or_path)\n",
        "print(classifier.model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxryyMD_seAE",
        "outputId": "81ac45b8-0f92-4335-cf3a-1302ff6d2cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: joy  score: 0.9848\n",
            "j-hartmann/emotion-english-distilroberta-base\n",
            "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเขียนโปรแกรมเชิงวัตถุ\n",
        "แนวคิดพื้นฐาน OOP\n",
        "1. Class / Template แม่พิมของวัตถุ\n",
        "2. Object / instance สิ่งที่ถูกสร้างจาก class\n",
        "3. Encapsulation สมบัติในการซ่อนรายละเอียดการทำงานเพราะ user ไม่รู้ว่า object นี้ทำงานอย่างไรรู้แค่ว่ามันมี method นี้\n",
        "4. inheritance การสืบทอดคุณสมบัติจากคลาสอื่น\n",
        "5. Polymorephism ความสามารถในการใช้ method กับวัตถุประเภทต่างๆ"
      ],
      "metadata": {
        "id": "2wSXGo08EyEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dog:\n",
        "  def __init__(self, name, breed):\n",
        "    self.name = name\n",
        "    self.breed = breed\n",
        "  '''\n",
        "  การใช้ method __init__ คือการ initiialize class และ self คือตัว object\n",
        "  เองแล้วใช้ self.something เพื่อ intialze attribute\n",
        "  '''\n",
        "  def bark(self):\n",
        "    print(self.name, \"say woof!\")\n",
        "  # method barkของ class Dog\n",
        "obj = Dog(\"cat\",\"thaicat\")\n",
        "obj.bark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwchqPuNE0_-",
        "outputId": "c287b4bc-3756-4991-d0c1-9b8bc4931042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat say woof!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ตัวอย่าง OOP ใน Hugging face\n",
        "1. AuTokenizer คลาสที่สร้าง tokenizer อันตโนมัติตามโมเดลที่เลือก\n",
        "2. AutoModel คลาสแม่ของโมเดลทั่วไป\n",
        "3. AutoModelForSequenceClassification โมเดลที่ใช้สำหรับงาน classification\n",
        "4. PreTrainedModel คลาสแม่ของทุกโมเดล"
      ],
      "metadata": {
        "id": "30aGFErGGI1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# method\n",
        "# ที่รับ parameter ที่ช่วยในการdownload model โดยไม่ต้องกำหนด ค่าทุกอย้่างเอง\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "223315d4d2d24ac5847a3930ef6b5add",
            "e92a125b2a874377827a8bd4538362d5",
            "f19045a25ea7484daf47fd4185a3afef",
            "0e063c9c4d8d4952bfc8edd856dcca40",
            "994f701ac2c44caf9a068f7335a7a968",
            "d1182cc09589496aaf3c7d6bd30189bd",
            "7ee3b6221b944534a422b0f2e03c210d",
            "0efd3067e4de46059987d114fd187566",
            "461bd200b82843d3b396cd3629101ea9",
            "d99cb66f7cdd4344b377db00dcfdf047",
            "5a3e00d720b6415aa3903e8a15e37a5d"
          ]
        },
        "id": "FGEljmbXGgTG",
        "outputId": "2782643b-5991-475a-e3d3-3a15404d84aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "223315d4d2d24ac5847a3930ef6b5add"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ถ้าไม่อยากจัดการ model เองใช้ pipeline ได้\n",
        "from transformers import pipeline\n",
        "classifier = pipeline('text-classification')\n",
        "print(classifier('I love using huggin face')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czJXcYayG4vY",
        "outputId": "c25432db-a9fe-4883-d8fd-bc2b5d4754b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9997461438179016}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การใช้งาน Gradio สร้าง Web UI\n"
      ],
      "metadata": {
        "id": "VuvdFzMqmu7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6_4EwjlmzRh",
        "outputId": "43764cef-8f6c-416f-89ea-91e56aac00dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.47.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Collecting gradio-client==1.13.3 (from gradio)\n",
            "  Downloading gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.47.2-py3-none-any.whl (60.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.13.0\n",
            "    Uninstalling gradio_client-1.13.0:\n",
            "      Successfully uninstalled gradio_client-1.13.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.46.0\n",
            "    Uninstalling gradio-5.46.0:\n",
            "      Successfully uninstalled gradio-5.46.0\n",
            "Successfully installed gradio-5.47.2 gradio-client-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('text-classification', model='distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
        "# print(classifier.model.name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIRAMK59m43z",
        "outputId": "ef041032-fe4e-44c3-9f44-8d8ad664fc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text):\n",
        "  result = classifier(text)[0]\n",
        "  return result['label'] + \" with score: \"+ str(round(result['score'],4))"
      ],
      "metadata": {
        "id": "VnW1BCUknPor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn=classify_text,\n",
        "                    inputs=gr.Textbox(label='Enter your text '),\n",
        "                    outputs=gr.Textbox(label='Sentiment result '),\n",
        "                    title=\"Sentiment analysis\")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "B-63VJs-nzf2",
        "outputId": "00a015fe-f6cf-490e-ea73-329d77a8dab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://68a69d60b7554d7cfc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://68a69d60b7554d7cfc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  ใช่้ Block\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gqmn7llupz-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('text-classification', model='distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "def classify_text(text, threshold):\n",
        "  result = classifier(text)[0]\n",
        "  label = result['label']\n",
        "  score = result['score']\n",
        "  if (score < threshold):\n",
        "    return \"Not confident in the result\"\n",
        "  return label + \" with score: \" + str(round(score,4))\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.Markdown(\"# Sentiment analysis system\")\n",
        "  with gr.Row():\n",
        "    text_input = gr.Textbox(label=\"Enter your text \")\n",
        "    slider = gr.Slider(minimum=0, maximum= 1, value=0.5, label=\"Confident Threshold\")\n",
        "  result_output = gr.Textbox(label='Result')\n",
        "  analyze_button = gr.Button(\"Analyze\")\n",
        "  analyze_button.click(fn=classify_text, inputs=[text_input, slider]\n",
        "                       ,outputs=result_output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "NYHc_a_ip6jv",
        "outputId": "a6b848fa-9bce-4b50-afd8-e7ce9dc7bf82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://09a29d607dc2c8cf24.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://09a29d607dc2c8cf24.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การใช้งาน Pipeline เบื้องต้น"
      ],
      "metadata": {
        "id": "wvN7ZcdX2e_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "xFfEkBl92iSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('sentiment-analysis')\n",
        "help(classifier) # ใช้ help method เพื่อดูรายละเอียดของ pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6nKghQEW2klv",
        "outputId": "2b39d7a5-207d-4120-d207-37a811b47ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on TextClassificationPipeline in module transformers.pipelines.text_classification object:\n",
            "\n",
            "class TextClassificationPipeline(transformers.pipelines.base.Pipeline)\n",
            " |  TextClassificationPipeline(**kwargs)\n",
            " |\n",
            " |  Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\n",
            " |  examples](../task_summary#sequence-classification) for more information.\n",
            " |\n",
            " |  Example:\n",
            " |\n",
            " |  ```python\n",
            " |  >>> from transformers import pipeline\n",
            " |\n",
            " |  >>> classifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
            " |  >>> classifier(\"This movie is disgustingly good !\")\n",
            " |  [{'label': 'POSITIVE', 'score': 1.0}]\n",
            " |\n",
            " |  >>> classifier(\"Director tried too much.\")\n",
            " |  [{'label': 'NEGATIVE', 'score': 0.996}]\n",
            " |  ```\n",
            " |\n",
            " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
            " |\n",
            " |  This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
            " |  `\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\n",
            " |\n",
            " |  If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\n",
            " |  over the results. If there is a single label, the pipeline will run a sigmoid over the result. In case of regression\n",
            " |  tasks (`model.config.problem_type == \"regression\"`), will not apply any function on the output.\n",
            " |\n",
            " |  The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\n",
            " |  the up-to-date list of available models on\n",
            " |  [huggingface.co/models](https://huggingface.co/models?filter=text-classification).\n",
            " |\n",
            " |  Arguments:\n",
            " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
            " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
            " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
            " |      tokenizer ([`PreTrainedTokenizer`]):\n",
            " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
            " |          [`PreTrainedTokenizer`].\n",
            " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
            " |          Model card attributed to the model for this pipeline.\n",
            " |      framework (`str`, *optional*):\n",
            " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
            " |          installed.\n",
            " |\n",
            " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
            " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
            " |          provided.\n",
            " |      task (`str`, defaults to `\"\"`):\n",
            " |          A task-identifier for the pipeline.\n",
            " |      num_workers (`int`, *optional*, defaults to 8):\n",
            " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
            " |          workers to be used.\n",
            " |      batch_size (`int`, *optional*, defaults to 1):\n",
            " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
            " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
            " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
            " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
            " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
            " |      device (`int`, *optional*, defaults to -1):\n",
            " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
            " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
            " |      dtype (`str` or `torch.dtype`, *optional*):\n",
            " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
            " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
            " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
            " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
            " |          the raw output data e.g. text.\n",
            " |      return_all_scores (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to return all prediction scores or just the one of the predicted class.\n",
            " |      function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
            " |          The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
            " |\n",
            " |          - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
            " |            has several labels, will apply the softmax function on the output. In case of regression tasks, will not\n",
            " |            apply any function on the output.\n",
            " |          - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
            " |          - `\"softmax\"`: Applies the softmax function on the output.\n",
            " |          - `\"none\"`: Does not apply any function on the output.\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      TextClassificationPipeline\n",
            " |      transformers.pipelines.base.Pipeline\n",
            " |      transformers.pipelines.base._ScikitCompat\n",
            " |      abc.ABC\n",
            " |      transformers.utils.hub.PushToHubMixin\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __call__(self, inputs: Union[str, list[str], dict[str, str], list[dict[str, str]]], **kwargs: Any) -> list[dict[str, typing.Any]]\n",
            " |      Classify the text(s) given as inputs.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs (`str` or `list[str]` or `dict[str]`, or `list[dict[str]]`):\n",
            " |              One or several texts to classify. In order to use text pairs for your classification, you can send a\n",
            " |              dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\n",
            " |          top_k (`int`, *optional*, defaults to `1`):\n",
            " |              How many results to return.\n",
            " |          function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
            " |              The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
            " |              values:\n",
            " |\n",
            " |              If this argument is not specified, then it will apply the following functions according to the number\n",
            " |              of labels:\n",
            " |\n",
            " |              - If problem type is regression, will not apply any function on the output.\n",
            " |              - If the model has a single label, will apply the sigmoid function on the output.\n",
            " |              - If the model has several labels, will apply the softmax function on the output.\n",
            " |\n",
            " |              Possible values are:\n",
            " |\n",
            " |              - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
            " |              - `\"softmax\"`: Applies the softmax function on the output.\n",
            " |              - `\"none\"`: Does not apply any function on the output.\n",
            " |\n",
            " |      Return:\n",
            " |          A list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
            " |\n",
            " |          - **label** (`str`) -- The label predicted.\n",
            " |          - **score** (`float`) -- The corresponding probability.\n",
            " |\n",
            " |          If `top_k` is used, one such dictionary is returned per label.\n",
            " |\n",
            " |  __init__(self, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  postprocess(self, model_outputs, function_to_apply=None, top_k=1, _legacy=True)\n",
            " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
            " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
            " |      numbers).\n",
            " |\n",
            " |  preprocess(self, inputs, **tokenizer_kwargs) -> dict[str, typing.Union[list['GenericTensor'], ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]]\n",
            " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
            " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {}\n",
            " |\n",
            " |  function_to_apply = <ClassificationFunction.NONE: 'none'>\n",
            " |\n",
            " |  return_all_scores = False\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
            " |\n",
            " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
            " |      Check if the model class is in supported by the pipeline.\n",
            " |\n",
            " |      Args:\n",
            " |          supported_models (`list[str]` or `dict`):\n",
            " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
            " |\n",
            " |  device_placement(self)\n",
            " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
            " |\n",
            " |      Returns:\n",
            " |          Context manager\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
            " |      pipe = pipeline(..., device=0)\n",
            " |      with pipe.device_placement():\n",
            " |          # Every framework specific tensor allocation will be done on the request device\n",
            " |          output = pipe(...)\n",
            " |      ```\n",
            " |\n",
            " |  ensure_tensor_on_device(self, **inputs)\n",
            " |      Ensure PyTorch tensors are on the specified device.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
            " |              The tensors to place on `self.device`.\n",
            " |          Recursive on lists **only**.\n",
            " |\n",
            " |      Return:\n",
            " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
            " |\n",
            " |  forward(self, model_inputs, **forward_params)\n",
            " |\n",
            " |  get_inference_context(self)\n",
            " |\n",
            " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
            " |\n",
            " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
            " |\n",
            " |  predict(self, X)\n",
            " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
            " |\n",
            " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
            " |      Upload the pipeline file to the 🤗 Model Hub.\n",
            " |\n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
            " |              when pushing to a given organization.\n",
            " |          use_temp_dir (`bool`, *optional*):\n",
            " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
            " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
            " |          token (`bool` or `str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
            " |              is not specified.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
            " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
            " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
            " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
            " |              Google Colab instances without any CPU OOM issues.\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
            " |          revision (`str`, *optional*):\n",
            " |              Branch to push the uploaded files to.\n",
            " |          commit_description (`str`, *optional*):\n",
            " |              The description of the commit that will be created\n",
            " |          tags (`list[str]`, *optional*):\n",
            " |              List of tags to push on the Hub.\n",
            " |\n",
            " |      Examples:\n",
            " |\n",
            " |      ```python\n",
            " |      from transformers import pipeline\n",
            " |\n",
            " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
            " |\n",
            " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
            " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
            " |\n",
            " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
            " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
            " |      ```\n",
            " |\n",
            " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
            " |\n",
            " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
            " |\n",
            " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)\n",
            " |      Save the pipeline's model and tokenizer.\n",
            " |\n",
            " |      Args:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
            " |          safe_serialization (`str`):\n",
            " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
            " |          kwargs (`dict[str, Any]`, *optional*):\n",
            " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |\n",
            " |  transform(self, X)\n",
            " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
            " |\n",
            " |  dtype\n",
            " |      Dtype of the model (if it's Pytorch model), `None` otherwise.\n",
            " |\n",
            " |  torch_dtype\n",
            " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
            " |\n",
            " |  default_input_names = None\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ]
    }
  ]
}