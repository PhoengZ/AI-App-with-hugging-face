{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation\n",
        "เช่น\n",
        "1.   การแต่งเรื่อง\n",
        "2.   การเขียนโค้ด\n",
        "3.   การเขียนข้อความ\n",
        "\n"
      ],
      "metadata": {
        "id": "gx9-mpGZ-hmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "sQfLpo6O-rYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "fc24dcd9c8984f1cb756f3dae18fc544",
            "e353940457d24d4ebde4fd60e429a653",
            "d99eee24197b419b907cbdac53f6c6e0",
            "6773f217426b4f488016ba16b0c3d05f",
            "553520867d1c4ad9a631af2ebb86c06e",
            "2adfe46f63604f2da0bde82dcb34b589",
            "a382edb44f0d435f9b3f4eaaa8920a7a",
            "a07233ca0009448a823d646344163e84",
            "a218da8623bb4fc38ff55489dd3237fe",
            "b59378d596b748018bb7795f128e601c",
            "741425834e6d43d295b77452bfc4c10a",
            "ff4f328c083d4ab38fc425e6ac91aa11",
            "17190e5e1d03459e82ae3bf364b66cbe",
            "01d5c3ce326f47c587d4bd4dc53c8263",
            "aa706273f754425e9d7ea89e1ed3c91c",
            "49498ce560fe464aaa0370d098685b8d",
            "78f0eee145144168ba0405c3cba3ea2d",
            "56224b80eca74358aad01bbe394ea4bf",
            "67eb5345de1c40b4a91256d35a145b4f",
            "1b5def8f006d4f74941a6ce54698bda4",
            "b5c91747d0a44a5eaea94a7bc8722bfe",
            "e125c2a14c394682a7a7b84ac5f93dc8",
            "80f338d5f51945a4827e5f339cb5f092",
            "3bf1326b612d4ab5b7b361193ff637a8",
            "1a245fa81d4d4fecb0f62a83db4159c9",
            "426a3df937a2490ea04510b6f7acac35",
            "eadc5cadc150490da0fa6d13e2b42e57",
            "c31135c1f4b84ec8bdcc308534ce74ec",
            "a51fd74fddab427abf9ed50ee1f88517",
            "bf1d8627ff2c4cf29418ec29f04a0921",
            "b1c5c3e52d534fa4a2aef694b862755b",
            "8d0d2651de6846588df19cb89b8c5444",
            "3b529ee20af74fa99cf496c2a22fa5a7",
            "d0ecd2b95de84fa491a7953ed8f4d3f2",
            "c9fe8d46cd3045c29e629a1cb1cb1bb8",
            "9349581847ff4f248c3c26a5cde04704",
            "dbe1e532d259489e8c267065c9d63426",
            "deb5e0a388424fa989d3e7ab8aecbcce",
            "cc9773c323704100a5db5a9f7cad3061",
            "cd9acdf32e284b6d91159b91ed6861f9",
            "eab305a359dc40e6b0c5f44f67eb7821",
            "a1c82ee1dba1484797691b9a859c1c0b",
            "93015c0bea8a49a9be339f2567fc7470",
            "e878c9baae3a4562a73fbeb56f3fe903",
            "dc95e7b89bfe4d1b93f0d9fa0e90b440",
            "6809ca14a82f4a8ab730624b497a6af7",
            "e45f7645eff242a4900f857d7fb67aa2",
            "fb63860e7725441dbf0304db65bf22ae",
            "0b3161ea24fb47beaef2082506fe034b",
            "0f5b2d4219b846eea39c589a96204479",
            "36996e5914c24f4881f2c2de7e234038",
            "dc9fef75cdc84a59b21ca19d28f0336a",
            "fc38cc14bd784a3b8817edc36cce9d85",
            "98346556aead4e17a0aca91833b7f66f",
            "3108b8e599f84b3e9c8d03d641cc7974",
            "144717fb313a4c78809e4e5d1ff98a74",
            "00c7bf12e4b5491d86881f688f812593",
            "5ef6fa45d6df474d94da221a60c58991",
            "451f6accccb34b6d8c025bcb4cc9ea4a",
            "de81a91e5bc049569ca52b9687116a04",
            "44f42cd8d9ff4ee7b89bb66855f8e119",
            "ffd543ba978a43d49846e39995c40d37",
            "8bae9384063945d6b4065727f5d17241",
            "b6307a45ce0448be9257342bad034998",
            "91597847f17141c4bc8a0cf2416bd49a",
            "feb7e154007b46caad9d989566ce44d7",
            "f3deadef3e354789aa0ccf38915a19b0",
            "8e852046ea78412f9fbbd39423253937",
            "e8bb4245791847f0a6fca83b9187e3f8",
            "47cfea5285f04c9eb7733b91f3b9c4c5",
            "d688694e516643a8bac8712ed06e5f7d",
            "f694652c6f754db29720a2c37e41a703",
            "d8b7c35282e940b9a3d8c3be026af0bb",
            "2d2604274d064526a447b451c9b7cb99",
            "b65e404609aa42d7a428f5f26795dcea",
            "1d28e1a0c2a14ad28e55c6054c6356f5",
            "325d08afbafe4ba1ae44ef52e2c1bca8"
          ]
        },
        "id": "GZI48tO4-4XX",
        "outputId": "c084eccc-65af-45c6-8fc1-94b912c92a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc24dcd9c8984f1cb756f3dae18fc544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4f328c083d4ab38fc425e6ac91aa11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80f338d5f51945a4827e5f339cb5f092"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0ecd2b95de84fa491a7953ed8f4d3f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc95e7b89bfe4d1b93f0d9fa0e90b440"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "144717fb313a4c78809e4e5d1ff98a74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3deadef3e354789aa0ccf38915a19b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from above huggin face will set auto model be gpt-2\n",
        "print(generator.model.name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4cfOdTA-8S_",
        "outputId": "8a78cbcf-e6a3-4b56-a04f-b88639d9bf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai-community/gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In a near future, AI\""
      ],
      "metadata": {
        "id": "xninO8o3_E3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the genration\n",
        "result = generator(prompt, do_sample= False)\n",
        "# กำหนด do_sample = False เพื่อบอกให้ model\n",
        "# ตอบคำถามที่ดีที่สุดเสมอไม่ต้องไม่ต้องมีการสุ่มเลือกให้หลากหลาย\n",
        "# ทำให้ result เป็น list ของคำคอบที่มีค่าซ้ำ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNT2ZXO3_Mu3",
        "outputId": "45206b31-a636-430a-ed71-330b4e8f0641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------------------\")\n",
        "print(\"\\n\".join(textwrap.wrap(result[0]['generated_text'])))\n",
        "print(\"-----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycANgzXy_dVm",
        "outputId": "832b4012-8fb3-48a1-8289-aa7d77a90561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "In a near future, AI will be able to predict the future and predict\n",
            "what will happen in the future.  The AI will be able to predict the\n",
            "future and predict what will happen in the future. The AI will be able\n",
            "to predict the future and predict what will happen in the future. The\n",
            "AI will be able to predict the future and predict what will happen in\n",
            "the future. The AI will be able to predict the future and predict what\n",
            "will happen in the future. The AI will be able to predict the future\n",
            "and predict what will happen in the future. The AI will be able to\n",
            "predict the future and predict what will happen in the future. The AI\n",
            "will be able to predict the future and predict what will happen in the\n",
            "future. The AI will be able to predict the future and predict what\n",
            "will happen in the future. The AI will be able to predict the future\n",
            "and predict what will happen in the future. The AI will be able to\n",
            "predict the future and predict what will happen in the future. The AI\n",
            "will be able to predict the future and predict what will happen in the\n",
            "future. The AI will be able to predict the future and predict what\n",
            "will happen in the future. The AI will be able to predict the future\n",
            "and predict what will happen in the future. The AI will be\n",
            "-----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If want to sample more answer\n",
        "generator = pipeline('text-generation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S-ilDVm_np3",
        "outputId": "e74c6466-019c-485a-e2db-f0ac3add6219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = generator(prompt, do_sample= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPHcS5Qc_-JX",
        "outputId": "b8eb45d1-0cf5-4c5c-d195-6d1251b29690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------------------\")\n",
        "print(\"\\n\".join(textwrap.wrap(result[0]['generated_text'])))\n",
        "print(\"-----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAgEH9wyAILn",
        "outputId": "1e2c5c07-c234-4ff9-c537-657b991a20dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "In a near future, AI could also make it easier to understand and to\n",
            "predict what is going to happen in the future, a scenario that has\n",
            "been speculated about for decades.  Theoretical physicist Stephen\n",
            "Hawking announced recently that he has started a project called 'Human\n",
            "Intelligence' to find the best way to communicate with the human\n",
            "brain, using artificial intelligence. The project, called\n",
            "'Intelligence', is based on the idea that artificial intelligence is\n",
            "one of the key discoveries of the past decades.  It is already in\n",
            "development and it has already been used widely.  AI has already made\n",
            "it easier to understand and to predict what is going to happen in the\n",
            "future, a scenario that has been speculated about for decades  It is\n",
            "already in development and it has already been used widely  The\n",
            "research has been led by Dr Richard J. Gage, an assistant professor of\n",
            "physics at the University of Washington who has long been a proponent\n",
            "of artificial intelligence for the future.  Dr Gage said: 'There's a\n",
            "lot of interest in artificial intelligence, to see whether it can be\n",
            "used for the future.  'There are a lot of questions about whether it's\n",
            "safe to use it for practical purposes.  'There are a lot of questions\n",
            "about whether it could come in\n",
            "-----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if want to assign length of generation text\n",
        "result = generator(prompt, do_sample= True, min_length=300,max_length=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq1S89OSAXLm",
        "outputId": "c6322dbb-75e1-4240-98a4-84ac5b782b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1618: UserWarning: Unfeasible length constraints: `min_length` (300) is larger than the maximum possible length (262). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------------------\")\n",
        "print(\"\\n\".join(textwrap.wrap(result[0]['generated_text'])))\n",
        "print(\"-----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdxKxJANAmwu",
        "outputId": "a2283c9f-1c12-4d5c-89dc-ac15b47a29cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "In a near future, AI will be able to learn from human memories in ways\n",
            "that are much easier to understand and understand.  I'm not sure how\n",
            "AI will be able to do that, but if you'd like to see how it works, a\n",
            "lot of the work is already done and is really easy to implement. For\n",
            "example, consider the following image. It's just a simple image, but\n",
            "at the same time it's a beautiful one. You can imagine the different\n",
            "layers of computer technology that AI could potentially be used to\n",
            "learn from. Imagine also the human brain, which is a machine learning\n",
            "system.  Why is it that we have so much trouble understanding the\n",
            "human brain?  The answer to this is very simple. The most basic\n",
            "question we ask in this question is what kind of computer system is\n",
            "working for us.  We have just begun to understand what kind of\n",
            "computer system is working for us. In other words, we're starting to\n",
            "understand what kind of computer system is working for us.  However,\n",
            "we're not sure how to build your own computer system.  What is the\n",
            "best way to build a computer system?  The best way to build a computer\n",
            "system is to be able to build a computer system from scratch. However,\n",
            "you can\n",
            "-----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If want to each round generated that generated 1 text to n text\n",
        "result = generator(prompt, do_sample= True, min_length=300,max_length=500, num_return_sequences=3)\n",
        "# from above we defined generated 3 text for each round generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qArmStOmAxQ0",
        "outputId": "bb6d0e6c-796f-45d9-80bd-1c55a7bafaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1618: UserWarning: Unfeasible length constraints: `min_length` (300) is larger than the maximum possible length (262). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in result:\n",
        "  print(\"-----------------------\")\n",
        "  print(\"\\n\".join(textwrap.wrap(r['generated_text'])))\n",
        "  print(\"-----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrLC6xWrBEkW",
        "outputId": "187f8e89-8ea4-4d4a-c96a-5a3036f37b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "In a near future, AI could be used to help people with diseases.  The\n",
            "\"smart\" AI, which can recognize your face, and then create a \"Smart\"\n",
            "smile to let people see you, could also help to save lives by helping\n",
            "people, according to the study.  \"People might be more prepared for a\n",
            "natural disaster,\" said study co-author Dr. Joseph M. Bremner, an\n",
            "assistant professor of psychology at the University of California,\n",
            "Santa Cruz.  The research involved 2,000 U.S. adults. Researchers used\n",
            "automated face recognition systems at 2,000 locations to perform face\n",
            "recognition tasks.  Researchers found that people who used a face\n",
            "recognition system at least once in the past 20 minutes were less\n",
            "likely to experience a heart attack, stroke or other death.\n",
            "Participants who did not use a face recognition system more than once\n",
            "were more likely to experience a heart attack, stroke or other death.\n",
            "\"This is a huge step forward,\" Bremner said.  This study was funded by\n",
            "a grant from the National Institutes of Health.  In the 2013 edition\n",
            "of the journal Nature Neuroscience, M. A. Schoenberg, a postdoctoral\n",
            "fellow at the University of California, Santa Cruz, and a co-author on\n",
            "the paper\n",
            "-----------------------\n",
            "-----------------------\n",
            "In a near future, AI's abilities will have to be enhanced to be able\n",
            "to perform tasks that are not feasible today, such as driving cars.\n",
            "But if the AI can be created with the right tools and AI technology,\n",
            "the technology could be applied to a wide range of other issues, or to\n",
            "anything in between.  This research is the first to look at the way AI\n",
            "can be applied to the problems of high-tech work. It provides a new\n",
            "way of thinking about how computers can be used to solve problems that\n",
            "are not easily solved.  The research also shows how computers could be\n",
            "used to make life easier for people with disabilities.  \"Our approach\n",
            "is based on the fact that computers can be used to change the world,\"\n",
            "said Dr Paddy Pritchard, a cognitive-behavioral psychologist at the\n",
            "University of Sussex.  \"We need to change how we think about\n",
            "technology and how we use computers.  \"We need to think about how\n",
            "computers can be used to help people with disabilities.\"  The research\n",
            "was published in the journal Nature Communications. The paper is\n",
            "titled: \"The Future of AI and Intelligent Machines: A Case Study in\n",
            "the Netherlands\".  Explore further: AI can help people with autism\n",
            "develop smarter driving skills  More information\n",
            "-----------------------\n",
            "-----------------------\n",
            "In a near future, AI could be deployed to replace human characters\n",
            "with artificial intelligence.  The idea is to let humans build\n",
            "computers and robots that could communicate with each other, and would\n",
            "use different rules for how they communicate.  For example, humans\n",
            "could communicate with each other via the internet.  \"It's similar to\n",
            "using the internet to send messages through the phone,\" said Rob Van\n",
            "Auken, professor of artificial intelligence at UC Berkeley.  \"We can\n",
            "talk to one another using a lot of different ways.  \"You can talk to\n",
            "two different people and you can talk to the same person, but you\n",
            "can't talk to the same person on a computer.  \"You can communicate\n",
            "with a computer of different sizes, but you can't communicate with two\n",
            "different people on a computer.\"  The idea is simple, and could be\n",
            "part of the next generation of AI.  But the team behind AI won't be\n",
            "able to do that on its own.  \"The technology will need to be developed\n",
            "and used within different industries,\" said Van Auken.  \"But we will\n",
            "need to build robots that can interact with each other, and\n",
            "communicate with each other via the internet.  \"It's not all about AI,\n",
            "but\n",
            "-----------------------\n"
          ]
        }
      ]
    }
  ]
}